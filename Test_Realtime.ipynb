{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model_lstm import AudioEventDetector\n",
    "from audioset import vggish_input, vggish_params, vggish_postprocess, vggish_slim\n",
    "# aed = AudioEventDetector()\n",
    "\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "import time\n",
    "\n",
    "#TODO: change the config name to correct config\n",
    "import config as config\n",
    "\n",
    "cfg = config.Config()\n",
    "class_mapping = {k:v for k,v in zip(range(cfg.NUM_CLASSES), cfg.CLASS_INDS)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRec(object):\n",
    "    # Class for Audio Recording Object\n",
    "    def __init__(self):\n",
    "        self.r = sr.Recognizer()\n",
    "        self.src = sr.Microphone()\n",
    "        with self.src as source:\n",
    "            print(\"Calibrating microphone...\")\n",
    "            self.r.adjust_for_ambient_noise(source, duration=2)\n",
    "\n",
    "\n",
    "    def listen(self, save_path):\n",
    "        with self.src as source:\n",
    "            print(\"Recording ...\")\n",
    "            # record for a maximum of 10s\n",
    "            audio = self.r.listen(source, phrase_time_limit=10)\n",
    "        # write audio to a WAV file\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGish(object):\n",
    "    # Class for initializing the VGGish from checkpoints and extracting embeddings\n",
    "    # Paths to downloaded VGGish files.\n",
    "    CHECKPOINT_PATH = 'audioset/vggish_model.ckpt'\n",
    "    PCA_PARAMS_PATH = 'audioset/vggish_pca_params.npz'\n",
    "\n",
    "    def __init__(self):\n",
    "        with tf.Graph().as_default():\n",
    "            self.sess = tf.Session()\n",
    "            vggish_slim.define_vggish_slim()\n",
    "            vggish_slim.load_vggish_slim_checkpoint(self.sess, self.CHECKPOINT_PATH)\n",
    "\n",
    "\n",
    "    def extract_features(self, wav_path):\n",
    "        # Produce a batch of log mel spectrogram examples.\n",
    "        input_batch = vggish_input.wavfile_to_examples(wav_path)\n",
    "        if input_batch.shape[0] < 1:\n",
    "            print('{}: Audio sample shorter than 1 second. Ignoring ...', os.path.basename(wav_path))\n",
    "            return None\n",
    "\n",
    "        features_tensor = self.sess.graph.get_tensor_by_name(\n",
    "                vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = self.sess.graph.get_tensor_by_name(\n",
    "                vggish_params.OUTPUT_TENSOR_NAME)\n",
    "        [embedding_batch] = self.sess.run([embedding_tensor],\n",
    "                                    feed_dict={features_tensor: input_batch})\n",
    "        # Postprocess the results to produce whitened quantized embeddings.\n",
    "        pproc = vggish_postprocess.Postprocessor(self.PCA_PARAMS_PATH)\n",
    "        postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "        return (np.float32(postprocessed_batch) - 128.) / 128.\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        print (\"Closing the session..\")\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(mini_batch):\n",
    "    \"\"\"Function to pad 0's to make input example atleast 10 frames long\"\"\"\n",
    "    batch = np.copy(mini_batch)\n",
    "    max_len = 10 # max 10 frames\n",
    "    feat_len = 128 # 128-d embeddings\n",
    "    seq_len = batch.shape[0]\n",
    "    if seq_len != max_len:\n",
    "        batch = np.vstack([batch, np.zeros((max_len - seq_len, feat_len), dtype=batch.dtype)])\n",
    "    return batch[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from audioset/vggish_model.ckpt\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 10, 128)           98816     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 100,106\n",
      "Trainable params: 100,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Calibrating microphone...\n",
      "Recording ...\n",
      "Extracted features of shape:  (10, 128)\n",
      "Top 3 predicted classes are:  ['Typing', 'Cough', 'Laughter']\n",
      "Recording ...\n",
      "Closing the session..\n",
      "Quitting application\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the VGGish, AudioEventDetector classifier and AudioRecorder objects\n",
    "vggish = VGGish()\n",
    "aed = AudioEventDetector(cfg.TRAINED_MODEL_PATH)\n",
    "aed.model.summary()\n",
    "ar = AudioRec()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        ar.listen(\"microphone-results.wav\")\n",
    "        features = vggish.extract_features(\"microphone-results.wav\")\n",
    "\n",
    "        if features is not None:\n",
    "            print (\"Extracted features of shape: \", features.shape)\n",
    "            features = pad_sequences(features)\n",
    "            scores = aed.predict(features)\n",
    "            pred = np.argsort(scores, axis=1)[0][-3:][::-1]\n",
    "            pred_classes = [cfg.CLASSES[class_mapping[i]] for i in pred]\n",
    "            print (\"Top 3 predicted classes are: \", pred_classes)\n",
    "        time.sleep(0.1)\n",
    "    except KeyboardInterrupt:\n",
    "        del vggish\n",
    "        print (\"Quitting application\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
